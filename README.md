Tarea 3 ‚Äì Sistemas Distribuidos
An√°lisis Ling√º√≠stico Offline con Hadoop y Pig
Descripci√≥n
Este proyecto corresponde al Entregable 3 del curso Sistemas Distribuidos. El objetivo de esta etapa es transicionar del procesamiento en tiempo real (Tarea 2) al an√°lisis offline (Batch) de grandes vol√∫menes de datos.


El sistema toma el hist√≥rico de respuestas almacenadas (tanto humanas como generadas por el LLM) y utiliza el ecosistema Apache Hadoop para realizar un an√°lisis ling√º√≠stico comparativo, identificando patrones de vocabulario y frecuencia de palabras mediante trabajos de MapReduce escritos en Apache Pig.


Arquitectura
El sistema se compone de los siguientes servicios containerizados:


Hadoop NameNode & DataNode: Proveen el sistema de archivos distribuido (HDFS) para almacenar los datasets de entrada y salida.

Pig Container: Act√∫a como cliente y procesador. Ejecuta los scripts de Pig Latin que transforman los datos.

Orquestador (Script Bash): Automatiza el ciclo de vida completo: espera al cl√∫ster, realiza la ingesta de datos y ejecuta el an√°lisis.

Flujo de Datos (Pipeline ETL): Archivos de Texto (.txt) ‚Üí Ingesta a HDFS ‚Üí Procesamiento MapReduce (Pig) ‚Üí Exportaci√≥n a CSV

L√≥gica de Procesamiento (MapReduce)
El script de Pig implementa las siguientes etapas de transformaci√≥n sobre los textos:

Tokenizaci√≥n: Separaci√≥n de respuestas en palabras individuales.

Normalizaci√≥n: Conversi√≥n a min√∫sculas y eliminaci√≥n de signos de puntuaci√≥n (Regex).

Filtrado (Stopwords): Eliminaci√≥n de palabras comunes sin valor sem√°ntico (ej: "the", "and", "is") mediante un Join con una lista de exclusi√≥n.

Agregaci√≥n: Conteo de frecuencia de palabras (WordCount) y ordenamiento descendente.

Ejecuci√≥n con Docker
El despliegue ha sido completamente automatizado. No se requieren comandos manuales de HDFS ni de Pig.

Requisitos
Docker y Docker Compose instalados.

Pasos
Bash

git clone https://github.com/bastianseguin-bit/Tarea-3-sistemas-distribuidos.git
cd Tarea-3-sistemas-distribuidos/batch_analysis
docker-compose up
¬øQu√© sucede al ejecutar esto?

Se levanta el cl√∫ster de Hadoop.

El contenedor pig ejecuta autom√°ticamente el script run_analysis.sh.

El script espera a que Hadoop salga del "Safe Mode".

Se suben los datasets (humanas.txt, llm.txt) y stopwords.txt a HDFS.

Se ejecutan los trabajos de an√°lisis.

Los resultados finales se descargan autom√°ticamente a tu carpeta local data/.

Resultados
Al finalizar la ejecuci√≥n (cuando veas el mensaje üéâ ¬°AN√ÅLISIS COMPLETADO!), encontrar√°s dos archivos en la carpeta data/:

resultado_humanas.csv: Top de palabras m√°s usadas por usuarios reales.

resultado_llm.csv: Top de palabras m√°s usadas por la IA.

Estos datos permiten comparar la riqueza l√©xica y formalidad entre humanos y m√°quinas.

Video de Demostraci√≥n
